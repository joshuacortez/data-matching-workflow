{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "# do this to be able to import the custom python scripts\n",
    "sys.path.insert(1, \"../../../python_scripts\")\n",
    "import os\n",
    "\n",
    "import dm_utils\n",
    "import dm_file_checker\n",
    "\n",
    "import dedupe\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Appropriate Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using recall value of 1.0\n",
      "Numeric fields are []\n"
     ]
    }
   ],
   "source": [
    "saved_files_path = \"../../../saved_files\"\n",
    "task_name = os.path.basename(os.getcwd())\n",
    "    \n",
    "unlabeled_data_filepath = dm_file_checker.get_proper_unlabeled_data_filepath(task_name, saved_files_path)\n",
    "settings_filepath =  dm_file_checker.get_filepath(task_name, \"model_settings\", saved_files_path)\n",
    "labeled_data_filepath = dm_file_checker.get_filepath(task_name, \"labeled_data\", saved_files_path)\n",
    "blocks_filepath = dm_file_checker.get_filepath(task_name, \"blocks\", saved_files_path)\n",
    "\n",
    "numeric_fields = dm_file_checker.get_dataset_info(task_name, \"numeric_fields\", saved_files_path)\n",
    "RECALL_TRAIN_VAL = dm_file_checker.get_task_info(task_name, \"recall_train\", saved_files_path)\n",
    "print(\"Using recall value of {}\".format(RECALL_TRAIN_VAL))\n",
    "print(\"Numeric fields are {}\".format(numeric_fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default blocked_proportion is 50%\n",
    "BLOCKED_PROPORTION = 0.5\n",
    "# default sample size is 15000\n",
    "#SAMPLE_SIZE = 30_000\n",
    "SAMPLE_SIZE = 15_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting 190 empty string values of column date_of_birth to None\n",
      "converting 156 empty string values of column given_name to None\n",
      "converting 79 empty string values of column surname to None\n",
      "converting 245 empty string values of column street_number to None\n",
      "converting 154 empty string values of column address_1 to None\n",
      "converting 693 empty string values of column address_2 to None\n",
      "converting 85 empty string values of column suburb to None\n",
      "converting 0 empty string values of column postcode to None\n",
      "converting 85 empty string values of column state to None\n",
      "converting 0 empty string values of column soc_sec_id to None\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data = dm_utils.read_unlabeled_data_json(unlabeled_data_filepath, numeric_fields = numeric_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "- Link on syntax on how to define fields https://docs.dedupe.io/en/latest/Variable-definition.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model if it Already Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a settings file already exists, we'll just load that and skip training\n",
    "if os.path.exists(settings_filepath):\n",
    "    print('reading from', settings_filepath)\n",
    "    with open(settings_filepath, 'rb') as f:\n",
    "        deduper = dedupe.StaticDedupe(f)\n",
    "    skip_training = True\n",
    "else:\n",
    "    skip_training = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Data Fields for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following fields for the model\n",
      "[{'field': 'street_number', 'type': 'ShortString', 'has missing': True}, {'field': 'postcode', 'type': 'ShortString'}, {'field': 'state', 'type': 'ShortString', 'has missing': True}, {'field': 'soc_sec_id', 'type': 'ShortString'}, {'field': 'given_name', 'type': 'String', 'has missing': True}, {'field': 'surname', 'type': 'String', 'has missing': True}, {'field': 'address_1', 'type': 'String', 'has missing': True}, {'field': 'address_2', 'type': 'String', 'has missing': True}, {'field': 'suburb', 'type': 'String', 'has missing': True}, {'field': 'date_of_birth', 'type': 'DateTime', 'has missing': True, 'fuzzy': False, 'yearfirst': True}]\n"
     ]
    }
   ],
   "source": [
    "if not skip_training:\n",
    "    fields = dm_file_checker.get_task_info(task_name, \"fields\", saved_files_path)\n",
    "    print(\"Using the following fields for the model\")\n",
    "    print(fields)\n",
    "\n",
    "    deduper = dedupe.Dedupe(fields, num_cores = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data in Model (NOTE: this might take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:reading training from file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading labeled examples from  ../../../saved_files/dedup-febrl3/training_output/labeled_data.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(SimplePredicate: (commonIntegerPredicate, address_2), SimplePredicate: (commonTwoTokens, suburb))\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:SimplePredicate: (sameThreeCharStartPredicate, soc_sec_id)\n",
      "INFO:dedupe.training:(SimplePredicate: (dayPredicate, date_of_birth), SimplePredicate: (hundredIntegersOddPredicate, postcode))\n",
      "INFO:dedupe.training:(SimplePredicate: (fingerprint, address_2), SimplePredicate: (fingerprint, given_name))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonThreeTokens, address_1), SimplePredicate: (fingerprint, given_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 53s, sys: 1.99 s, total: 1min 55s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not skip_training:\n",
    "    dm_utils.prepare_training_deduper(deduper, unlabeled_data, labeled_data_filepath, \n",
    "                                      blocked_proportion = BLOCKED_PROPORTION, \n",
    "                                      sample_size = SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling of Data Proper\n",
    "\n",
    "## Watch Out For the Following When Tagging\n",
    "1. Family Members\n",
    "    - Family members usually have many fields in common (last name, address, landline number, place of birth, mothers maiden name). Key distinguishing fields are first name, birthdate, social security number (SSS), and tax identification number (TIN)\n",
    "    - Most tricky are young siblings because only distinguishing fields would be first name and birthdate. No SSS and TIN because they're not yet of working age.\n",
    "2. Businesses Mistagged as Individuals\n",
    "    - If there's a pair of records with the same social security number (SSS) and tax identification number (TIN) but one of the records has no first name, middle name, and last name, then one of the records may be a business mistagged as an individual.\n",
    "    - Still treat these records as separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting active labeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "street_number : 3\n",
      "postcode : 2101\n",
      "state : vic\n",
      "soc_sec_id : 1092994\n",
      "given_name : holly\n",
      "surname : green\n",
      "address_1 : larpent street\n",
      "address_2 : brentwood vlge\n",
      "suburb : ormond\n",
      "date_of_birth : 1941/01/14\n",
      "\n",
      "street_number : 3\n",
      "postcode : 2101\n",
      "state : vic\n",
      "soc_sec_id : 8051408\n",
      "given_name : matthew\n",
      "surname : green\n",
      "address_1 : larpent sztreet\n",
      "address_2 : brentwodo vlge\n",
      "suburb : diane lla\n",
      "date_of_birth : None\n",
      "\n",
      "66/10 positive, 60/10 negative\n",
      "Do these records refer to the same thing?\n",
      "(y)es / (n)o / (u)nsure / (f)inished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished labeling\n",
      "INFO:rlr.crossvalidation:using cross validation to find optimum alpha...\n",
      "INFO:rlr.crossvalidation:optimum alpha: 0.000100, score 0.9534646324666324\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:SimplePredicate: (sameThreeCharStartPredicate, soc_sec_id)\n",
      "INFO:dedupe.training:(SimplePredicate: (dayPredicate, date_of_birth), SimplePredicate: (hundredIntegersOddPredicate, postcode))\n",
      "INFO:dedupe.training:(SimplePredicate: (fingerprint, address_2), SimplePredicate: (fingerprint, given_name))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonThreeTokens, address_1), SimplePredicate: (fingerprint, given_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.7 s, sys: 994 ms, total: 26.7 s\n",
      "Wall time: 30 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not skip_training:\n",
    "    # ## Active learning\n",
    "    # Dedupe will find the next pair of records\n",
    "    # it is least certain about and ask you to label them as duplicates\n",
    "    # or not.\n",
    "    # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "    # press 'f' when you are finished\n",
    "    print('starting active labeling...')\n",
    "    dedupe.console_label(deduper)\n",
    "\n",
    "    # Using the examples we just labeled, train the deduper and learn blocking predicates\n",
    "    deduper.train(recall = RECALL_TRAIN_VAL)\n",
    "    \n",
    "    dm_utils.save_trained_deduper(deduper, labeled_data_filepath, settings_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Model if Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain_model:\n",
    "    for field in deduper.fingerprinter.index_fields:\n",
    "        field_data = set(record[field] for key,record in unlabeled_data.items())\n",
    "        deduper.fingerprinter.index(field_data, field)\n",
    "   \n",
    "    dedupe.console_label(deduper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain_model:\n",
    "\n",
    "    # Using the examples we just labeled, train the deduper and learn blocking predicates\n",
    "    deduper.train(recall = RECALL_TRAIN_VAL)\n",
    "    \n",
    "    dm_utils.save_trained_deduper(deduper, labeled_data_filepath, settings_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Blocks to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_utils.write_deduper_blocks(deduper, unlabeled_data, blocks_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance of blocking method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidate record pairs after blocking is 21,011\n",
      "Reduction ratio from blocking is 99.915956%\n"
     ]
    }
   ],
   "source": [
    "blocked_data = dm_utils.read_deduper_blocks(unlabeled_data, blocks_filepath)\n",
    "num_candidate_pairs = dm_utils.count_blocked_pairs(deduper, blocked_data)\n",
    "reduction_ratio = 1 - (num_candidate_pairs/(len(unlabeled_data)**2))\n",
    "\n",
    "print(\"Number of candidate record pairs after blocking is {:,}\".format(num_candidate_pairs))\n",
    "print(\"Reduction ratio from blocking is {}%\".format(reduction_ratio*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of top 10 biggest blocks are: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Record pair contributions from top 10 biggest blocks are : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "blocked_data = dm_utils.read_deduper_blocks(unlabeled_data, blocks_filepath)\n",
    "dm_utils.check_block_sizes(blocked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_matching] *",
   "language": "python",
   "name": "conda-env-data_matching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
