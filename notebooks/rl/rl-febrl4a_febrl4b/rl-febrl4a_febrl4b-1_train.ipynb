{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "# do this to be able to import the custom python scripts\n",
    "sys.path.insert(1, \"../../../python_scripts\")\n",
    "import os\n",
    "\n",
    "import dm_utils\n",
    "import dm_file_checker\n",
    "\n",
    "import dedupe\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Appropriate Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using recall value of 1.0\n"
     ]
    }
   ],
   "source": [
    "saved_files_path = \"../../../saved_files\"\n",
    "task_name = os.path.basename(os.getcwd())\n",
    "    \n",
    "is_data_source_deduped = dm_file_checker.check_is_data_source_deduped(task_name, saved_files_path)\n",
    "    \n",
    "# files to be written out\n",
    "settings_filepath =  dm_file_checker.get_filepath(task_name, \"model_settings\", saved_files_path)\n",
    "labeled_data_filepath = dm_file_checker.get_filepath(task_name, \"labeled_data\", saved_files_path)\n",
    "blocks_filepath = dm_file_checker.get_filepath(task_name, \"blocks\", saved_files_path)\n",
    "\n",
    "RECALL_TRAIN_VAL = dm_file_checker.get_task_info(task_name, \"recall_train\", saved_files_path)\n",
    "print(\"Using recall value of {}\".format(RECALL_TRAIN_VAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_data_source_deduped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default blocked_proportion is 50%\n",
    "BLOCKED_PROPORTION = 0.5\n",
    "# default sample size is 15000\n",
    "#SAMPLE_SIZE = 30_000\n",
    "SAMPLE_SIZE = 15_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using the canonicalized deduped dataset. Using the original preprocessed data.\n",
      "Numeric fields 1 are []\n",
      "Numeric fields 2 are []\n",
      "converting 94 empty string values of column date_of_birth to None\n",
      "converting 112 empty string values of column given_name to None\n",
      "converting 48 empty string values of column surname to None\n",
      "converting 158 empty string values of column street_number to None\n",
      "converting 98 empty string values of column address_1 to None\n",
      "converting 420 empty string values of column address_2 to None\n",
      "converting 55 empty string values of column suburb to None\n",
      "converting 0 empty string values of column postcode to None\n",
      "converting 50 empty string values of column state to None\n",
      "converting 0 empty string values of column soc_sec_id to None\n",
      "converting 263 empty string values of column date_of_birth to None\n",
      "converting 234 empty string values of column given_name to None\n",
      "converting 102 empty string values of column surname to None\n",
      "converting 287 empty string values of column street_number to None\n",
      "converting 220 empty string values of column address_1 to None\n",
      "converting 851 empty string values of column address_2 to None\n",
      "converting 106 empty string values of column suburb to None\n",
      "converting 0 empty string values of column postcode to None\n",
      "converting 107 empty string values of column state to None\n",
      "converting 0 empty string values of column soc_sec_id to None\n"
     ]
    }
   ],
   "source": [
    "if is_data_source_deduped:\n",
    "    unlabeled_data_1, unlabeled_data_2 = dm_utils.get_deduped_data_for_rl(task_name, saved_files_path)\n",
    "    print(\"Using canonicalized deduped dataset instead of the original preprocessed data.\")\n",
    "else:\n",
    "    unlabeled_data_1_filepath, unlabeled_data_2_filepath = dm_file_checker.get_proper_unlabeled_data_filepath(task_name, saved_files_path)\n",
    "    print(\"Not using the canonicalized deduped dataset. Using the original preprocessed data.\")\n",
    "    \n",
    "    numeric_fields_1, numeric_fields_2 = dm_file_checker.get_dataset_info(task_name, \"numeric_fields\", saved_files_path)\n",
    "    print(\"Numeric fields 1 are {}\".format(numeric_fields_1))\n",
    "    print(\"Numeric fields 2 are {}\".format(numeric_fields_2))\n",
    "    \n",
    "    unlabeled_data_1 = dm_utils.read_unlabeled_data_json(unlabeled_data_1_filepath, numeric_fields = numeric_fields_1)\n",
    "    unlabeled_data_2 = dm_utils.read_unlabeled_data_json(unlabeled_data_2_filepath, numeric_fields = numeric_fields_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "- Link on syntax on how to define fields https://docs.dedupe.io/en/latest/Variable-definition.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model if it Already Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If a settings file already exists, we'll just load that and skip training\n",
    "if os.path.exists(settings_filepath):\n",
    "    print('reading from', settings_filepath)\n",
    "    with open(settings_filepath, 'rb') as f:\n",
    "        linker = dedupe.StaticRecordLink(f)\n",
    "    skip_training = True\n",
    "else:\n",
    "    skip_training = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Data Fields for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following fields for the model\n",
      "[{'field': 'street_number', 'type': 'ShortString', 'has missing': True}, {'field': 'postcode', 'type': 'ShortString'}, {'field': 'state', 'type': 'ShortString', 'has missing': True}, {'field': 'soc_sec_id', 'type': 'ShortString'}, {'field': 'given_name', 'type': 'String', 'has missing': True}, {'field': 'surname', 'type': 'String', 'has missing': True}, {'field': 'address_1', 'type': 'String', 'has missing': True}, {'field': 'address_2', 'type': 'String', 'has missing': True}, {'field': 'suburb', 'type': 'String', 'has missing': True}, {'field': 'date_of_birth', 'type': 'DateTime', 'has missing': True, 'fuzzy': False, 'yearfirst': True}]\n"
     ]
    }
   ],
   "source": [
    "if not skip_training:\n",
    "    fields = dm_file_checker.get_task_info(task_name, \"fields\", saved_files_path)\n",
    "    print(\"Using the following fields for the model\")\n",
    "    print(fields)\n",
    "\n",
    "    linker = dedupe.RecordLink(fields, num_cores = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data in Model (NOTE: this might take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:reading training from file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading labeled examples from  ../../../saved_files/rl-febrl4a_febrl4b/training_output/labeled_data.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.canopy_index:Removing stop word ce\n",
      "INFO:dedupe.canopy_index:Removing stop word re\n",
      "INFO:dedupe.canopy_index:Removing stop word  s\n",
      "INFO:dedupe.canopy_index:Removing stop word ee\n",
      "INFO:dedupe.canopy_index:Removing stop word st\n",
      "INFO:dedupe.canopy_index:Removing stop word tr\n",
      "INFO:dedupe.canopy_index:Removing stop word et\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(SimplePredicate: (commonTwoTokens, address_2), SimplePredicate: (suffixArray, given_name))\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(LevenshteinSearchPredicate: (2, postcode), LevenshteinSearchPredicate: (2, soc_sec_id))\n",
      "INFO:dedupe.training:(SimplePredicate: (dayPredicate, date_of_birth), TfidfTextSearchPredicate: (0.2, address_1))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonIntegerPredicate, address_2), SimplePredicate: (sortedAcronym, address_1))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonIntegerPredicate, address_2), SimplePredicate: (commonTwoTokens, suburb))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonThreeTokens, suburb), TfidfNGramSearchPredicate: (0.4, surname))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonThreeTokens, address_2), SimplePredicate: (twoGramFingerprint, surname))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 11s, sys: 3.97 s, total: 7min 15s\n",
      "Wall time: 7min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not skip_training:\n",
    "    dm_utils.prepare_training_linker(linker, unlabeled_data_1, unlabeled_data_2,\n",
    "                                      labeled_data_filepath, \n",
    "                                      blocked_proportion = BLOCKED_PROPORTION, \n",
    "                                      sample_size = SAMPLE_SIZE)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling of Data Proper\n",
    "\n",
    "## Watch Out For the Following When Tagging\n",
    "1. Family Members\n",
    "    - Family members usually have many fields in common (last name, address, landline number, place of birth, mothers maiden name). Key distinguishing fields are first name, birthdate, social security number (SSS), and tax identification number (TIN)\n",
    "    - Most tricky are young siblings because only distinguishing fields would be first name and birthdate. No SSS and TIN because they're not yet of working age.\n",
    "2. Businesses Mistagged as Individuals\n",
    "    - If there's a pair of records with the same social security number (SSS) and tax identification number (TIN) but one of the records has no first name, middle name, and last name, then one of the records may be a business mistagged as an individual.\n",
    "    - Still treat these records as separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting active labeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "street_number : 26\n",
      "postcode : 3042\n",
      "state : tas\n",
      "soc_sec_id : 8370048\n",
      "given_name : kirra\n",
      "surname : browne\n",
      "address_1 : shoalhaven avenue\n",
      "address_2 : loormeah park\n",
      "suburb : surry hills\n",
      "date_of_birth : 1943/04/07\n",
      "\n",
      "street_number : 26\n",
      "postcode : 3042\n",
      "state : tas\n",
      "soc_sec_id : 8221881\n",
      "given_name : kirra\n",
      "surname : browne\n",
      "address_1 : shoalhaven avenue\n",
      "address_2 : loormeah park\n",
      "suburb : surry hills\n",
      "date_of_birth : None\n",
      "\n",
      "78/10 positive, 61/10 negative\n",
      "Do these records refer to the same thing?\n",
      "(y)es / (n)o / (u)nsure / (f)inished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished labeling\n",
      "INFO:rlr.crossvalidation:using cross validation to find optimum alpha...\n",
      "INFO:rlr.crossvalidation:optimum alpha: 0.000010, score 0.9431560492122969\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(LevenshteinSearchPredicate: (2, postcode), LevenshteinSearchPredicate: (2, soc_sec_id))\n",
      "INFO:dedupe.training:(SimplePredicate: (dayPredicate, date_of_birth), TfidfTextSearchPredicate: (0.2, address_1))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonIntegerPredicate, address_2), SimplePredicate: (sortedAcronym, address_1))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonIntegerPredicate, address_2), SimplePredicate: (commonTwoTokens, suburb))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonThreeTokens, suburb), TfidfNGramSearchPredicate: (0.4, surname))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonThreeTokens, address_2), SimplePredicate: (twoGramFingerprint, surname))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 53s, sys: 2.15 s, total: 1min 55s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not skip_training:\n",
    "    # ## Active learning\n",
    "    # Dedupe will find the next pair of records\n",
    "    # it is least certain about and ask you to label them as duplicates\n",
    "    # or not.\n",
    "    # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "    # press 'f' when you are finished\n",
    "    print('starting active labeling...')\n",
    "    dedupe.console_label(linker)\n",
    "\n",
    "    # Using the examples we just labeled, train the deduper and learn blocking predicates\n",
    "    linker.train(recall = RECALL_TRAIN_VAL)\n",
    "    \n",
    "    dm_utils.save_trained_linker(linker, labeled_data_filepath, settings_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain Model if Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain_model:\n",
    "    dedupe.console_label(linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain_model:\n",
    "    for field in linker.fingerprinter.index_fields:\n",
    "        field_data_1 = set(record[field] for key,record in unlabeled_data_1.items())\n",
    "        field_data_2 = set(record[field] for key,record in unlabeled_data_2.items())\n",
    "        field_data = field_data_1.union(field_data_2)\n",
    "        linker.fingerprinter.index(field_data, field)\n",
    "        \n",
    "    # Using the examples we just labeled, train the deduper and learn blocking predicates\n",
    "    linker.train(recall = RECALL_TRAIN_VAL)\n",
    "    \n",
    "    dm_utils.save_trained_linker(linker, labeled_data_filepath, settings_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Blocks to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_utils.write_linker_blocks(linker, unlabeled_data_1, unlabeled_data_2, blocks_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance of blocking method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidate record pairs after blocking is 5,659\n",
      "Reduction ratio from blocking is 99.97736400000001%\n"
     ]
    }
   ],
   "source": [
    "blocked_data = dm_utils.read_linker_blocks(unlabeled_data_1, unlabeled_data_2, blocks_filepath)\n",
    "num_candidate_pairs = dm_utils.count_blocked_pairs(linker, blocked_data)\n",
    "reduction_ratio = 1 - (num_candidate_pairs/(len(unlabeled_data_1)*len(unlabeled_data_2)))\n",
    "\n",
    "print(\"Number of candidate record pairs after blocking is {:,}\".format(num_candidate_pairs))\n",
    "print(\"Reduction ratio from blocking is {}%\".format(reduction_ratio*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_matching] *",
   "language": "python",
   "name": "conda-env-data_matching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
